{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a061a475-539a-49b2-a793-4d54d371cff5",
        "_uuid": "2d418ae2-5cf9-4ee2-be21-887cbd12c033",
        "id": "8CsHPbUfJ2ym",
        "trusted": true
      },
      "source": [
        "# SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "9a49de34-2d0c-4cb6-8d25-36999973b84b",
        "_uuid": "9f8ddafc-6c56-4f57-b839-2e5b993c54b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-09-22T02:29:27.201419Z",
          "iopub.status.busy": "2022-09-22T02:29:27.200513Z",
          "iopub.status.idle": "2022-09-22T02:29:28.595614Z",
          "shell.execute_reply": "2022-09-22T02:29:28.594097Z",
          "shell.execute_reply.started": "2022-09-22T02:29:27.201257Z"
        },
        "id": "OL82Y4rBjZIV",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "b7f0bc79-486e-4335-bb69-9bba883904ea",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Automatic1111's Gradio Web UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "f299b5ef-ba76-4833-8dc5-64e96020cd63",
        "_uuid": "56bdb038-f02b-473c-bb83-9acd5fd62c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-09-22T02:29:45.244627Z",
          "iopub.status.busy": "2022-09-22T02:29:45.243987Z",
          "iopub.status.idle": "2022-09-22T02:29:47.949777Z",
          "shell.execute_reply": "2022-09-22T02:29:47.948273Z",
          "shell.execute_reply.started": "2022-09-22T02:29:45.244568Z"
        },
        "id": "sBbcB4vwj_jm",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "2aa990a6-1cc1-4a76-d234-d3c0c90c1b32",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/AUTOMATIC1111/stable-diffusion-webui\n",
        "%cd stable-diffusion-webui"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "5b481bfc-e47f-457a-80d8-c2f8f566f51b",
        "_uuid": "00b05248-2fe7-42a1-a709-23e9da863dc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2022-09-22T02:29:47.953209Z",
          "iopub.status.busy": "2022-09-22T02:29:47.952704Z",
          "iopub.status.idle": "2022-09-22T02:32:49.677263Z",
          "shell.execute_reply": "2022-09-22T02:32:49.675549Z",
          "shell.execute_reply.started": "2022-09-22T02:29:47.953160Z"
        },
        "id": "SaAJk33ppFw1",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "c978dd65-f374-42dc-bc54-7bf959dc3721",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt\n",
        "!mkdir repositories\n",
        "!git clone --depth 1 https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion\n",
        "!git clone --depth 1 https://github.com/CompVis/taming-transformers.git repositories/taming-transformers\n",
        "!git clone --depth 1 https://github.com/CompVis/latent-diffusion.git repositories/latent-diffusion\n",
        "!git clone --depth 1 https://github.com/sczhou/CodeFormer.git repositories/CodeFormer\n",
        "!git clone --depth 1 https://github.com/crowsonkb/k-diffusion.git repositories/k-diffusion\n",
        "!git clone --depth 1 https://github.com/salesforce/BLIP.git repositories/BLIP\n",
        "%pip install -r repositories/CodeFormer/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download SD v1.4 model from Huggingface\n",
        "You'll need a token from HuggingFace for this, and will need to agree to the T&C of the repo.\n",
        "\n",
        "Alternatively, upload it to your drive / storage and copy it over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "431bfd96-1f9a-4c64-b044-cae1ea8ae1e3",
        "_uuid": "26644eb5-b753-44d0-8cbe-327ea1a94b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2022-09-22T02:32:49.680603Z",
          "iopub.status.busy": "2022-09-22T02:32:49.680044Z",
          "iopub.status.idle": "2022-09-22T02:35:15.440804Z",
          "shell.execute_reply": "2022-09-22T02:35:15.439058Z",
          "shell.execute_reply.started": "2022-09-22T02:32:49.680502Z"
        },
        "id": "hrhHGwmDipjc",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "f7fad5d7-392c-4dfe-b9b3-7832b482c360",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!mkdir models\n",
        "!wget -c --header=\"Authorization: Bearer Your_Token_Here\" \"https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\" -O models/model.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## GFPGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "d6b8f61c-df11-4e99-9187-9e1cccb717de",
        "_uuid": "b84810c5-b259-4193-a637-19f2554484e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2022-09-22T02:35:15.445252Z",
          "iopub.status.busy": "2022-09-22T02:35:15.444102Z",
          "iopub.status.idle": "2022-09-22T02:35:40.223859Z",
          "shell.execute_reply": "2022-09-22T02:35:40.222371Z",
          "shell.execute_reply.started": "2022-09-22T02:35:15.445197Z"
        },
        "id": "mstv-L61KsF6",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1d35b58f-b333-4ab4-eedc-1b2d7ad83b56",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!wget -c https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/GFPGANv1.4.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use GPU to load models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "dfe39687-cc64-4326-a1e8-4df91b117776",
        "_uuid": "e36b6f05-bc5a-450d-9c8c-889619999ad9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-09-22T02:35:42.140642Z",
          "iopub.status.busy": "2022-09-22T02:35:42.140251Z",
          "iopub.status.idle": "2022-09-22T02:35:42.157755Z",
          "shell.execute_reply": "2022-09-22T02:35:42.155942Z",
          "shell.execute_reply.started": "2022-09-22T02:35:42.140607Z"
        },
        "id": "I_p_WUxT74Tp",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "4d887a4f-f9d4-47fe-f8f4-d1ec641bb112",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%cd stable-diffusion-webui\n",
        "def replace_line(file_name, line_num, text):\n",
        "    lines = open(file_name, 'r').readlines()\n",
        "    lines[line_num] = text\n",
        "    out = open(file_name, 'w')\n",
        "    out.writelines(lines)\n",
        "    out.close()\n",
        "\n",
        "def replace_line_in_file(file_name, line_to_search, text_to_replace):\n",
        "    with open(file_name, 'r') as file:\n",
        "        # read a list of lines into data\n",
        "        data = file.readlines()\n",
        "\n",
        "    for line in data:\n",
        "        # if the line contains the string we're looking for,\n",
        "        # write the line to the output file\n",
        "        if line_to_search in line:\n",
        "            replace_line(file_name, data.index(line), text_to_replace)\n",
        "\n",
        "replace_line_in_file('modules/sd_models.py', 'pl_sd = torch.load(checkpoint_file, map_location=\"cpu\")', '    pl_sd = torch.load(checkpoint_file, map_location=\"cuda:0\")\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "583cd295-0587-4e04-b6e7-89d79a56158f",
        "_uuid": "f54d131f-3729-477e-8397-a60c2d328e2b",
        "id": "jr9TOxYkT4YO",
        "trusted": true
      },
      "source": [
        "### Change maximum steps to 500\n",
        "Optional.  I personally find 150 to be very limiting.\n",
        "\n",
        "Also augments img2img steps to 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "e51be631-137a-484d-9f4f-60d54609aa0e",
        "_uuid": "f8d27d9c-9522-419d-8c86-d8c7a0e04107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-09-22T02:35:42.161633Z",
          "iopub.status.busy": "2022-09-22T02:35:42.160479Z",
          "iopub.status.idle": "2022-09-22T02:35:43.842951Z",
          "shell.execute_reply": "2022-09-22T02:35:43.841450Z",
          "shell.execute_reply.started": "2022-09-22T02:35:42.161493Z"
        },
        "id": "7Si02Xqh1gbf",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "4d1a933a-2159-4e48-8f79-864e7449c98d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!wget http://0x0.st/oWuq.json -O /content/stable-diffusion-webui/ui-config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIxT3HA0mA2O"
      },
      "source": [
        "### Downloading Models from HF with wget\n",
        "Replace 'username' and 'dataset' with whatever you wish to download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE_4jK-WmA2O",
        "outputId": "63ce895d-047d-4824-eed7-643e9d4459dd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#!wget -c --header=\"Authorization: Bearer Your_Token_Here\" \"https://huggingface.co/username/dataset/resolve/main/dataset.ckpt\" -O models/mymodel.ckpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6Q2SQXemA2R"
      },
      "source": [
        "### Download concurrently\n",
        "To save time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2022-09-22T02:29:35.772309Z",
          "iopub.status.busy": "2022-09-22T02:29:35.771086Z",
          "iopub.status.idle": "2022-09-22T02:29:45.241249Z",
          "shell.execute_reply": "2022-09-22T02:29:45.239583Z",
          "shell.execute_reply.started": "2022-09-22T02:29:35.772272Z"
        },
        "id": "ORdS9U1OmA2D",
        "outputId": "b68c1cc2-8bd5-40eb-e3ea-3704161427c3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#!apt -y install parallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E784f7IvmA2R",
        "outputId": "51352439-0724-4db0-ee5c-a5b7ab50a6c1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#!echo '!wget -c --header=\"Authorization: Bearer Your_Token_Here\" \"https://huggingface.co/username/dataset/resolve/main/dataset.ckpt\" -O models/mymodel.ckpt' > dllist\n",
        "#!echo '!wget -c --header=\"Authorization: Bearer Your_Token_Here\" \"https://huggingface.co/username/dataset/resolve2/main/dataset.ckpt\" -O models/mymodel2.ckpt' >> dllist\n",
        "#!cat dllist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn3I3x12mA2R",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#cat dllist | parallel "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If installing xformers takes too long, use the provided wheel (`.whl` file) here.  Will work for a T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U --pre triton\n",
        "%pip install git+https://github.com/facebookresearch/xformers@1d31a3a#egg=xformers\n",
        "# %pip install https://github.com/metrolobo/xformers_wheels/releases/download/1d31a3ac/xformers-0.0.14.dev0-cp37-cp37m-linux_x86_64.whl\n",
        "# Use the above for a much faster install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overwrite files to make use of xformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A61yR7wQEnju",
        "outputId": "365cfd5e-ee6d-4ee1-c1dc-bbbd5efecafd"
      },
      "outputs": [],
      "source": [
        "%%writefile /content/stable-diffusion-webui/repositories/stable-diffusion/ldm/modules/attention.py\n",
        "import gc\n",
        "from inspect import isfunction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat\n",
        "import os\n",
        "from typing import Any, Optional\n",
        "import xformers\n",
        "import xformers.ops\n",
        "\n",
        "\n",
        "from ldm.modules.diffusionmodules.util import checkpoint\n",
        "\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "\n",
        "def uniq(arr):\n",
        "    return{el: True for el in arr}.keys()\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def max_neg_value(t):\n",
        "    return -torch.finfo(t.dtype).max\n",
        "\n",
        "\n",
        "def init_(tensor):\n",
        "    dim = tensor.shape[-1]\n",
        "    std = 1 / math.sqrt(dim)\n",
        "    tensor.uniform_(-std, std)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "\n",
        "# feedforward\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = default(dim_out, dim)\n",
        "        project_in = nn.Sequential(\n",
        "            nn.Linear(dim, inner_dim),\n",
        "            nn.GELU()\n",
        "        ) if not glu else GEGLU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            project_in,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(inner_dim, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n",
        "        k = k.softmax(dim=-1)\n",
        "        context = torch.einsum('bhdn,bhen->bhde', k, v)\n",
        "        out = torch.einsum('bhde,bhdn->bhen', context, q)\n",
        "        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class SpatialSelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
        "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
        "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
        "\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
        "        w_ = rearrange(w_, 'b i j -> b j i')\n",
        "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
        "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, query_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "\n",
        "        q_in = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k_in = self.to_k(context)\n",
        "        v_in = self.to_v(context)\n",
        "        del context, x\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q_in, k_in, v_in))\n",
        "        del q_in, k_in, v_in\n",
        "\n",
        "        r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device)\n",
        "\n",
        "        stats = torch.cuda.memory_stats(q.device)\n",
        "        mem_active = stats['active_bytes.all.current']\n",
        "        mem_reserved = stats['reserved_bytes.all.current']\n",
        "        mem_free_cuda, _ = torch.cuda.mem_get_info(torch.cuda.current_device())\n",
        "        mem_free_torch = mem_reserved - mem_active\n",
        "        mem_free_total = mem_free_cuda + mem_free_torch\n",
        "\n",
        "        gb = 1024 ** 3\n",
        "        tensor_size = q.shape[0] * q.shape[1] * k.shape[1] * q.element_size()\n",
        "        modifier = 3 if q.element_size() == 2 else 2.5\n",
        "        mem_required = tensor_size * modifier\n",
        "        steps = 1\n",
        "\n",
        "\n",
        "        if mem_required > mem_free_total:\n",
        "            steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n",
        "            # print(f\"Expected tensor size:{tensor_size/gb:0.1f}GB, cuda free:{mem_free_cuda/gb:0.1f}GB \"\n",
        "            #      f\"torch free:{mem_free_torch/gb:0.1f} total:{mem_free_total/gb:0.1f} steps:{steps}\")\n",
        "\n",
        "        if steps > 64:\n",
        "            max_res = math.floor(math.sqrt(math.sqrt(mem_free_total / 2.5)) / 8) * 64\n",
        "            raise RuntimeError(f'Not enough memory, use lower resolution (max approx. {max_res}x{max_res}). '\n",
        "                               f'Need: {mem_required/64/gb:0.1f}GB free, Have:{mem_free_total/gb:0.1f}GB free')\n",
        "\n",
        "        slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n",
        "        for i in range(0, q.shape[1], slice_size):\n",
        "            end = i + slice_size\n",
        "            s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k) * self.scale\n",
        "\n",
        "            s2 = s1.softmax(dim=-1, dtype=q.dtype)\n",
        "            del s1\n",
        "\n",
        "            r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n",
        "            del s2\n",
        "\n",
        "        del q, k, v\n",
        "\n",
        "        r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n",
        "        del r1\n",
        "\n",
        "        return self.to_out(r2)\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=True):\n",
        "        super().__init__()\n",
        "        AttentionBuilder = MemoryEfficientCrossAttention        \n",
        "        self.attn1 = AttentionBuilder(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  # is a self-attention\n",
        "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
        "        self.attn2 = AttentionBuilder(query_dim=dim, context_dim=context_dim,\n",
        "                                    heads=n_heads, dim_head=d_head, dropout=dropout)  # is self-attn if context is none\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.checkpoint = checkpoint\n",
        "        \n",
        "    def _set_attention_slice(self, slice_size):\n",
        "        self.attn1._slice_size = slice_size\n",
        "        self.attn2._slice_size = slice_size\n",
        "\n",
        "    def forward(self, hidden_states, context=None):\n",
        "        hidden_states = hidden_states.contiguous() if hidden_states.device.type == \"mps\" else hidden_states\n",
        "        hidden_states = self.attn1(self.norm1(hidden_states)) + hidden_states\n",
        "        hidden_states = self.attn2(self.norm2(hidden_states), context=context) + hidden_states\n",
        "        hidden_states = self.ff(self.norm3(hidden_states)) + hidden_states\n",
        "        return hidden_states        \n",
        "\n",
        "    # def forward(self, x, context=None):\n",
        "        # return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
        "\n",
        "    # def _forward(self, x, context=None):\n",
        "        # x = self.attn1(self.norm1(x)) + x\n",
        "        # x = self.attn2(self.norm2(x), context=context) + x\n",
        "        # x = self.ff(self.norm3(x)) + x\n",
        "        # return x\n",
        "\n",
        "class MemoryEfficientCrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n",
        "        self.attention_op: Optional[Any] = None\n",
        "\n",
        "    def _maybe_init(self, x):\n",
        "        \"\"\"\n",
        "        Initialize the attention operator, if required We expect the head dimension to be exposed here, meaning that x\n",
        "        : B, Head, Length\n",
        "        \"\"\"\n",
        "        if self.attention_op is not None:\n",
        "            return\n",
        "\n",
        "        _, M, K = x.shape\n",
        "        try:\n",
        "            self.attention_op = xformers.ops.AttentionOpDispatch(\n",
        "                dtype=x.dtype,\n",
        "                device=x.device,\n",
        "                k=K,\n",
        "                attn_bias_type=type(None),\n",
        "                has_dropout=False,\n",
        "                kv_len=M,\n",
        "                q_len=M,\n",
        "            ).op\n",
        "\n",
        "        except NotImplementedError as err:\n",
        "            raise NotImplementedError(f\"Please install xformers with the flash attention / cutlass components.\\n{err}\")\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "        \n",
        "\n",
        "\n",
        "        b, _, _ = q.shape\n",
        "        q, k, v = map(\n",
        "            lambda t: t.unsqueeze(3)\n",
        "            .reshape(b, t.shape[1], self.heads, self.dim_head)\n",
        "            .permute(0, 2, 1, 3)\n",
        "            .reshape(b * self.heads, t.shape[1], self.dim_head)\n",
        "            .contiguous(),\n",
        "            (q, k, v),\n",
        "        )\n",
        "\n",
        "        # init the attention op, if required, using the proper dimensions\n",
        "        self._maybe_init(q)\n",
        "\n",
        "        # actually compute the attention, what we cannot get enough of\n",
        "        out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=self.attention_op)\n",
        "\n",
        "        # TODO: Use this directly in the attention operation, as a bias\n",
        "        if exists(mask):\n",
        "            raise NotImplementedError\n",
        "        out = (\n",
        "            out.unsqueeze(0)\n",
        "            .reshape(b, self.heads, out.shape[1], self.dim_head)\n",
        "            .permute(0, 2, 1, 3)\n",
        "            .reshape(b, out.shape[1], self.heads * self.dim_head)\n",
        "        )\n",
        "\n",
        "        stats = torch.cuda.memory_stats(q.device)\n",
        "        mem_active = stats['active_bytes.all.current']\n",
        "        mem_reserved = stats['reserved_bytes.all.current']\n",
        "        mem_free_cuda, _ = torch.cuda.mem_get_info(torch.cuda.current_device())\n",
        "        mem_free_torch = mem_reserved - mem_active\n",
        "        mem_free_total = mem_free_cuda + mem_free_torch\n",
        "\n",
        "        gb = 1024 ** 3\n",
        "        tensor_size = q.shape[0] * q.shape[1] * k.shape[1] * q.element_size()\n",
        "        modifier = 3 if q.element_size() == 2 else 2.5\n",
        "        mem_required = tensor_size * modifier\n",
        "        steps = 1\n",
        "\n",
        "\n",
        "        if mem_required > mem_free_total:\n",
        "            steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n",
        "            # print(f\"Expected tensor size:{tensor_size/gb:0.1f}GB, cuda free:{mem_free_cuda/gb:0.1f}GB \"\n",
        "            #      f\"torch free:{mem_free_torch/gb:0.1f} total:{mem_free_total/gb:0.1f} steps:{steps}\")\n",
        "\n",
        "        if steps > 64:\n",
        "            max_res = math.floor(math.sqrt(math.sqrt(mem_free_total / 2.5)) / 8) * 64\n",
        "            raise RuntimeError(f'Not enough memory, use lower resolution (max approx. {max_res}x{max_res}). '\n",
        "                               f'Need: {mem_required/64/gb:0.1f}GB free, Have:{mem_free_total/gb:0.1f}GB free')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block for image-like data.\n",
        "    First, project the input (aka embedding)\n",
        "    and reshape to b, t, d.\n",
        "    Then apply standard transformer action.\n",
        "    Finally, reshape to image\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, n_heads, d_head,\n",
        "                 depth=1, dropout=0., context_dim=None):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        inner_dim = n_heads * d_head\n",
        "        self.norm = Normalize(in_channels)\n",
        "\n",
        "        self.proj_in = nn.Conv2d(in_channels,\n",
        "                                 inner_dim,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim)\n",
        "                for d in range(depth)]\n",
        "        )\n",
        "\n",
        "        self.proj_out = zero_module(nn.Conv2d(inner_dim,\n",
        "                                              in_channels,\n",
        "                                              kernel_size=1,\n",
        "                                              stride=1,\n",
        "                                              padding=0))\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # note: if no context is given, cross-attention defaults to self-attention\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context=context)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "25ab9d9d-956c-42b0-9797-b48ea7f8950f",
        "_uuid": "f983aa2b-8505-4805-9468-f8653074fa9b",
        "id": "nbt5X29pe9NT",
        "trusted": true
      },
      "source": [
        "### Connect a Google drive to be able to save results to it.\n",
        "You can change the output folder path in the settings of the Gradio UI. \n",
        "\n",
        "Try `/content/drive/MyDrive/StableDiffusionOutputs`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "ea0951a0-077c-4ae2-a1f5-c0ee7509c52f",
        "_uuid": "beafcb65-66d0-4cd0-b25d-8b52f8d9c9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVf1aJNFe8E9",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "92f55ac2-b74d-46b5-d308-ec03282032ff",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f9238649-27f2-4bb0-a71c-1531caad5c77",
        "_uuid": "77ab3a79-c6b4-4a88-95ea-b8aee1f562a4",
        "id": "xt8lbdmC04ox",
        "trusted": true
      },
      "source": [
        "# Launch web ui.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "cfbc9b2a-4c68-47c4-87ac-5ed928fbad77",
        "_kg_hide-output": true,
        "_uuid": "2b0fb8cb-2b84-4225-b253-0754a08d63a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2022-09-22T02:40:34.850705Z",
          "iopub.status.busy": "2022-09-22T02:40:34.850136Z"
        },
        "id": "R-xAdMA5wxXd",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "a0c72a9f-b269-4263-d4de-217002dfba47",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "%cd /content/stable-diffusion-webui\n",
        "!python webui.py --share"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notes\n",
        "\n",
        "## Sources Used:\n",
        "\n",
        "Modified from https://github.com/daswer123/stable-diffusion-colab/blob/main/StableDiffusionUI_Voldemort.ipynb\n",
        "\n",
        "xformers added from https://github.com/TheLastBen/fast-stable-diffusion/blob/main/fast_stable_diffusion_AUTOMATIC1111.ipynb and https://github.com/ShivamShrirao/diffusers/blob/main/examples/dreambooth/train_dreambooth.py\n",
        "\n",
        "## Other platforms\n",
        "\n",
        "This notebook can run on platforms other than Google Colab.  Comment out all the google drive parts, and edit the file manually instead of using the `writefile` cell magic.\n",
        "\n",
        "If you don't have drive to save your outputs, you can use an S3 bucket on platforms that allow you to access `fusermount`.\n",
        "\n",
        "Alternatively `tar` your output folder and send it over with `croc` or `magic-wormhole`."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
